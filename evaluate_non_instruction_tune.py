# -*- coding: utf-8 -*-
"""evaluate_non_instruction_tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oyFT_oYKvD3SoOr41bo07C8FADezQi5V
"""

import os
import json
import torch
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm
from unsloth import FastLanguageModel
from transformers import BitsAndBytesConfig
import random

def set_seed(seed):
    """Set random seed for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def is_bfloat16_supported():
    """Check if bfloat16 is supported by the current hardware."""
    return torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8

def extract_malnutrition_decision(response):
    """
    Extract decision and explanation from model response.
    
    Args:
        response (str): Full model response
    
    Returns:
        tuple: (decision, explanation)
    """
    response = response.lower()
    decision = 1 if "yes" in response or "malnutrition" in response else 0
    return decision, response

class MalnutritionPromptBuilder:
    def __init__(self, examples_data=None):
        """
        Initialize prompt builder.
        
        Args:
            examples_data (str, optional): Path to examples CSV. Defaults to None.
        """
        self.examples_data = examples_data
        self.few_shot_examples = self._load_few_shot_examples() if examples_data else []

    def _load_few_shot_examples(self):
        """
        Load few-shot examples from CSV.
        
        Returns:
            list: List of few-shot examples
        """
        # Placeholder implementation
        return []

    def get_inference_prompt(self, patient_notes, note_col=None, label_col=None, 
                             num_examples=0, balanced=False):
        """
        Build prompt for model inference.
        
        Args:
            patient_notes (str): Patient notes to classify
            note_col (str, optional): Column name for notes. Defaults to None.
            label_col (str, optional): Column name for labels. Defaults to None.
            num_examples (int, optional): Number of few-shot examples. Defaults to 0.
            balanced (bool, optional): Whether to balance examples. Defaults to False.
        
        Returns:
            str: Formatted prompt
        """
        # Basic prompt template
        prompt = f"""Determine if the following patient notes indicate malnutrition:

Patient Notes:
{patient_notes}

Your response should:
1. Clearly state "Yes" or "No" about the presence of malnutrition
2. Provide a brief explanation of your reasoning
3. Base your decision on clinical indicators such as:
   - Weight status
   - Growth patterns
   - Physical examination findings
   - Nutritional intake
   - Presence of underlying conditions affecting nutrition

Respond with: "[Decision: Yes/No] Explanation: [Your reasoning]"
"""
        return prompt

def get_quantization_config(config):
    """Define quantization configuration for the model based on arguments."""
    # Determine if we should use 8-bit or 4-bit quantization (but not both)
    if config['load_in_8bit']:
        return BitsAndBytesConfig(
            load_in_8bit=True,
            load_in_4bit=False,
            llm_int8_enable_fp32_cpu_offload=True
        )
    elif config['load_in_4bit']:
        # Determine compute dtype based on available hardware and args
        if config['force_bf16'] and is_bfloat16_supported():
            compute_dtype = torch.bfloat16
        else:
            compute_dtype = torch.float16

        return BitsAndBytesConfig(
            load_in_4bit=True,
            load_in_8bit=False,
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            llm_int8_enable_fp32_cpu_offload=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=True
        )
    else:
        # No quantization
        return None

def get_device():
    """
    Get the appropriate device, prioritizing GPU usage.

    Returns:
        torch.device: The device to use for model inference
    """
    if torch.cuda.is_available():
        # Use CUDA GPU
        device = torch.device("cuda")
        print(f"GPU detected: {torch.cuda.get_device_name(0)}")
        print(
            f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        return device
    else:
        print("No GPU detected, falling back to CPU. This may significantly slow down inference.")
        return torch.device("cpu")

def load_model_and_tokenizer(base_model, model_path, config):
    """
    Load model and tokenizer for evaluation with appropriate quantization settings.
    """
    # Get device
    device = get_device()
    print(f"Using device: {device}")

    print(
        f"Loading {'fine-tuned' if model_path else 'base'} model: {base_model}")
    # Determine compute dtype based on hardware and preferences
    compute_dtype = torch.bfloat16 if is_bfloat16_supported(
    ) and not config['force_fp16'] else torch.float16
    # Get appropriate quantization config
    quantization_config = get_quantization_config(config)
    try:
        # Set up model loading kwargs with common parameters
        model_kwargs = {
            # Force GPU usage if available instead of "auto" mapping
            "device_map": "cuda" if torch.cuda.is_available() and not config['force_cpu'] else "auto",
            "use_flash_attention_2": config['use_flash_attention'],
            "use_cache": True  # Enable caching for inference
        }
        # Add quantization settings
        if quantization_config is not None:
            model_kwargs["quantization_config"] = quantization_config
        else:
            # Direct quantization flags if no config is provided
            model_kwargs["load_in_4bit"] = config['load_in_4bit']
            model_kwargs["load_in_8bit"] = config['load_in_8bit']
        # Set dtype for the model
        model_kwargs["dtype"] = compute_dtype
        if model_path:
            # Load base model with adapter weights (fine-tuned model)
            model_kwargs["model_name"] = base_model
            model_kwargs["adapter_name"] = model_path
            model, tokenizer = FastLanguageModel.from_pretrained(
                **model_kwargs)
            print(
                f"Model loaded successfully with adapter weights from {model_path}")
        else:
            # Load base model only
            model_kwargs["model_name"] = base_model
            model, tokenizer = FastLanguageModel.from_pretrained(
                **model_kwargs)
            print(f"Base model loaded successfully: {base_model}")
        # Enable native 2x faster inference
        FastLanguageModel.for_inference(model)
        return model, tokenizer
    except Exception as e:
        print(f"Error loading model: {e}")
        raise

def get_probability_from_logits(logits, tokenizer):
    """
    Extract probability of the 'yes' answer from model logits.
    """
    try:
        # Get token IDs for "yes" and "no"
        yes_tokens = tokenizer("yes", add_special_tokens=False).input_ids
        no_tokens = tokenizer("no", add_special_tokens=False).input_ids

        # Take the first token ID as representative
        yes_token_id = yes_tokens[0] if yes_tokens else tokenizer.encode("yes")[0]
        no_token_id = no_tokens[0] if no_tokens else tokenizer.encode("no")[0]

        # Extract the relevant logits
        yes_logit = logits[0, yes_token_id].item()
        no_logit = logits[0, no_token_id].item()

        # Convert to probabilities using softmax
        probs = np.exp([yes_logit, no_logit]) / np.sum(np.exp([yes_logit, no_logit]))
        yes_prob = probs[0]
        return yes_prob
    except Exception as e:
        print(f"Error calculating probability from logits: {e}")
        # Fallback to a default probability of 0.5 in case of errors
        return 0.5

def process_batch(batch_texts, model, tokenizer, prompt_builder, config):
    """
    Process a batch of patient notes and extract predictions and probabilities.
    """
    # Explicitly use GPU if available
    device = torch.device("cuda") if torch.cuda.is_available() and not config['force_cpu'] else torch.device("cpu")

    batch_results = []

    # First calculate probabilities for the entire batch
    batch_probs = []
    for text in batch_texts:
        # Prepare prompt with few-shot examples if specified
        prompt = prompt_builder.get_inference_prompt(
            patient_notes=text,
            note_col=config['text_column'],
            label_col=config['label_column'],
            num_examples=config['few_shot_count'],
            balanced=config['balanced_examples']
        )

        # Prepare chat format messages
        messages = [
            {"role": "user", "content": prompt}
        ]

        try:
            # Apply chat template to get input tokens and move to device
            inputs = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(device)

            # Get logits for the first token prediction
            with torch.no_grad():
                outputs = model(inputs)
                logits = outputs.logits[:, -1, :]

                # Get probability for "yes" class
                yes_probability = get_probability_from_logits(logits, tokenizer)
                batch_probs.append(yes_probability)

        except Exception as e:
            print(f"Error calculating probability: {e}")
            batch_probs.append(0.5)  # Default probability

    # Now generate the full responses
    for i, text in enumerate(batch_texts):
        try:
            prompt = prompt_builder.get_inference_prompt(
                patient_notes=text,
                note_col=config['text_column'],
                label_col=config['label_column'],
                num_examples=config['few_shot_count'],
                balanced=config['balanced_examples']
            )

            # Prepare chat format messages
            messages = [
                {"role": "user", "content": prompt}
            ]

            # Apply chat template to get input tokens and move to device
            inputs = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(device)

            # Generate the full response
            with torch.no_grad():
                output = model.generate(
                    input_ids=inputs,
                    max_new_tokens=config['max_new_tokens'],
                    temperature=config['temperature'],
                    do_sample=config['temperature'] > 0,
                    use_cache=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            # Decode the output - only the generated part
            input_length = inputs.shape[1]
            response_tokens = output[0][input_length:]
            response = tokenizer.decode(response_tokens, skip_special_tokens=True)

            # Extract decision and explanation
            decision, explanation = extract_malnutrition_decision(response)

            # Get the probability from the batch_probs
            yes_probability = batch_probs[i]

            # Apply threshold to convert probability to binary label (1/0)
            binary_decision = 1 if yes_probability >= config['threshold'] else 0

            batch_results.append((binary_decision, explanation, yes_probability))

        except Exception as e:
            print(f"Error generating response for item {i}: {e}")
            # Return default values in case of error
            batch_results.append((0, "Error processing text", 0.0))

    return batch_results

def evaluate_model(config, model, tokenizer, prompt_builder):
    """
    Evaluate model on test dataset using batch processing.
    """
    print(f"Loading test data from {config['test_csv']}")
    # Load test data
    df = pd.read_csv(config['test_csv'])

    # Handle ID column
    if config['id_column'] not in df.columns:
        print(f"ID column '{config['id_column']}' not found, creating it.")
        df[config['id_column']] = [f"patient_{i}" for i in range(len(df))]

    # Validate required columns exist
    required_columns = [config['text_column'], config['label_column'], config['id_column']]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        raise ValueError(f"Required columns {missing_columns} not found in test CSV")

    # Initialize results storage
    results = []

    # Prepare batches for processing
    batch_size = min(config['batch_size'], len(df))
    print(f"Processing dataset in batches of {batch_size}")

    # Create batches
    num_batches = (len(df) + batch_size - 1) // batch_size

    for batch_idx in tqdm(range(num_batches), desc="Processing batches"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(df))

        batch_df = df.iloc[start_idx:end_idx]

        # Get texts and other information for the batch
        batch_texts = []
        batch_ids = []
        batch_true_labels = []

        for _, row in batch_df.iterrows():
            patient_text = row[config['text_column']]
            patient_id = row[config['id_column']]

            # Handle missing text
            if pd.isna(patient_text) or patient_text == "":
                print(f"Warning: Empty text for patient {patient_id}, skipping.")
                continue

            # Convert true label to binary (1/0)
            true_label_str = str(row[config['label_column']]).lower()
            true_label = 1 if true_label_str in ["1", "yes", "true"] else 0

            batch_texts.append(patient_text)
            batch_ids.append(patient_id)
            batch_true_labels.append(true_label)

        # Process the batch if there are valid texts
        if batch_texts:
            batch_results = process_batch(batch_texts, model, tokenizer, prompt_builder, config)

            # Store results
            for i in range(len(batch_texts)):
                try:
                    predicted_label, explanation, probability = batch_results[i]

                    results.append({
                        "patient_id": batch_ids[i],
                        "true_label": batch_true_labels[i],
                        "predicted_label": predicted_label,
                        "probability": probability,
                        "explanation": explanation
                    })
                except Exception as e:
                    print(f"Error processing result for item {i} in batch {batch_idx}: {e}")
                    # Add error record
                    results.append({
                        "patient_id": batch_ids[i],
                        "true_label": batch_true_labels[i],
                        "predicted_label": -1,
                        "probability": 0.0,
                        "explanation": f"Error: {str(e)}"
                    })

    # Convert to DataFrame
    results_df = pd.DataFrame(results)

    # Filter out error rows for evaluation
    eval_df = results_df[results_df["predicted_label"] != -1].copy()

    if len(eval_df) == 0:
        print("WARNING: No valid results to evaluate! Check logs for details.")

    return results_df

def evaluate_predictions(y_true, y_pred, y_prob):
    """
    Compute evaluation metrics for predictions.
    
    Args:
        y_true (list): True labels
        y_pred (list): Predicted labels
        y_prob (list): Predicted probabilities
    
    Returns:
        dict: Evaluation metrics
    """
    from sklearn.metrics import (
        accuracy_score, 
        precision_score, 
        recall_score, 
        f1_score, 
        roc_auc_score, 
        average_precision_score, 
        confusion_matrix, 
        classification_report
    )

    # Compute metrics
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'auc': roc_auc_score(y_true, y_prob),
        'avg_precision': average_precision_score(y_true, y_prob),
        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),
        'classification_report': classification_report(y_true, y_pred, output_dict=True)
    }

    return metrics

def plot_evaluation_metrics(metrics, output_dir):
    """
    Generate evaluation metric plots and save to output directory.
    
    Args:
        metrics (dict): Evaluation metrics
        output_dir (str): Directory to save plots
    """
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Set style
    plt.style.use('seaborn')

    # Confusion Matrix Plot
    plt.figure(figsize=(8, 6))
    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))
    plt.close()

def save_metrics_to_csv(metrics, metrics_csv_path):
    """
    Save metrics to a CSV file.
    
    Args:
        metrics (dict): Evaluation metrics
        metrics_csv_path (str): Path to save CSV
    """
    # Convert metrics to DataFrame
    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])
    metrics_df.index.name = 'Metric'
    metrics_df.reset_index(inplace=True)
    
    # Save to CSV
    metrics_df.to_csv(metrics_csv_path, index=False)

def print_metrics_report(metrics):
    """
    Print a formatted report of evaluation metrics.
    
    Args:
        metrics (dict): Evaluation metrics
    """
    print("\n--- Model Evaluation Report ---")
    print(f"Accuracy:        {metrics['accuracy']:.4f}")
    print(f"Precision:       {metrics['precision']:.4f}")
    print(f"Recall:          {metrics['recall']:.4f}")
    print(f"F1 Score:        {metrics['f1']:.4f}")
    print(f"AUC:             {metrics['auc']:.4f}")
    print(f"Average Precision: {metrics['avg_precision']:.4f}")
    print("\nConfusion Matrix:")
    for row in metrics['confusion_matrix']:
        print(row)
    print("\nClassification Report:")
    print(metrics['classification_report'])

def main():
    """Run the evaluation pipeline using the given configuration dictionary."""
    config = {
        # Model arguments
        "model_path": "llama_3_2",
        "base_model": "unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit",

        # Test data arguments
        "test_csv": "data/test.csv",
        "text_column": "txt",
        "id_column": "DEID",
        "label_column": "label",

        # Few-shot settings
        "examples_data": None,
        "few_shot_count": 0,
        "balanced_examples": False,

        # Output arguments
        "output_dir": "./llama_3_2_post_non_instruct",
        "print_report": True,
        "save_report": True,

        # Quantization options
        "load_in_8bit": False,
        "load_in_4bit": True,

        # Model settings
        "use_flash_attention": False,
        "seed": 42,
        "max_new_tokens": 1024,
        "temperature": 0.0,
        "batch_size": 16,
        "threshold": 0.5,

        # Force precision options
        "force_fp16": False,
        "force_bf16": False,

        # Device arguments
        "force_cpu": False,
        "gpu_id": 0,
    }

    # Set seed for reproducibility
    set_seed(config["seed"])

    # Print CUDA information
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available() and not config["force_cpu"]:
        print(f"CUDA device count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"CUDA device {i}: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"  - Memory: {props.total_memory / 1024**3:.2f} GB")
            print(f"  - CUDA Capability: {props.major}.{props.minor}")

        # Set specific GPU to be the current device
        selected_gpu = min(config["gpu_id"], torch.cuda.device_count() - 1)
        torch.cuda.set_device(selected_gpu)
        print(f"Current CUDA device set to: {selected_gpu} ({torch.cuda.get_device_name(selected_gpu)})")

        # Clear GPU cache to maximize available memory
        torch.cuda.empty_cache()
        print("GPU cache cleared")

        # Verify GPU is being used
        device = torch.device("cuda", selected_gpu)
        print(f"Using device: {device}")

        # Optimize CUDA operations
        torch.backends.cudnn.benchmark = True
        print("cuDNN benchmark mode enabled for faster training")
    else:
        device = torch.device("cpu")
        print(f"Using device: {device}")
        if torch.cuda.is_available():
            print("Note: GPU is available but not being used due to force_cpu flag")

    # Create output directory
    os.makedirs(config["output_dir"], exist_ok=True)

    # Initialize prompt builder
    try:
        prompt_builder = MalnutritionPromptBuilder(config["examples_data"])
    except Exception as e:
        print(f"Error initializing prompt builder: {e}")
        raise

    # Load model and tokenizer with appropriate quantization settings
    try:
        model, tokenizer = load_model_and_tokenizer(
            config["base_model"], config["model_path"], config
        )
    except Exception as e:
        print(f"Error loading model and tokenizer: {e}")
        raise

    # Evaluate model on test data
    try:
        results_df = evaluate_model(config, model, tokenizer, prompt_builder)
    except Exception as e:
        print(f"Error during model evaluation: {e}")
        raise

    # Save predictions to CSV
    predictions_path = os.path.join(config["output_dir"], "predictions.csv")
    results_df.to_csv(predictions_path, index=False)
    print(f"Predictions saved to {predictions_path}")

    # Filter out error rows for evaluation
    eval_df = results_df[results_df["predicted_label"] != -1].copy()

    if len(eval_df) == 0:
        print("ERROR: No valid results to evaluate! Check logs for details.")
        return

    # Evaluate model performance
    y_true = eval_df["true_label"].tolist()
    y_pred = eval_df["predicted_label"].tolist()
    y_prob = eval_df["probability"].tolist()

    print("Computing evaluation metrics...")
    try:
        metrics = evaluate_predictions(y_true, y_pred, y_prob)
    except Exception as e:
        print(f"Error computing evaluation metrics: {e}")
        raise

    # Generate and save plots
    print("Generating evaluation plots...")
    try:
        plot_evaluation_metrics(metrics, config["output_dir"])
    except Exception as e:
        print(f"Error generating evaluation plots: {e}")

    # Save metrics to CSV
    try:
        metrics_csv_path = os.path.join(config["output_dir"], "metrics.csv")
        save_metrics_to_csv(metrics, metrics_csv_path)
        print(f"Metrics saved to {metrics_csv_path}")
    except Exception as e:
        print(f"Error saving metrics to CSV: {e}")

    # Save threshold and metrics to JSON
    metrics_json = {
        'threshold': config["threshold"],
        'accuracy': float(metrics['accuracy']),
        'precision': float(metrics['precision']),
        'recall': float(metrics['recall']),
        'f1': float(metrics['f1']),
        'auc': float(metrics['auc']),
        'avg_precision': float(metrics['avg_precision']),
        'confusion_matrix': metrics['confusion_matrix'],
        'classification_report': metrics['classification_report']
    }

    try:
        metrics_json_path = os.path.join(config["output_dir"], "metrics.json")
        with open(metrics_json_path, 'w') as f:
            json.dump(metrics_json, f, indent=2)
        print(f"Detailed metrics saved to {metrics_json_path}")
    except Exception as e:
        print(f"Error saving metrics to JSON: {e}")

    # Print evaluation report
    if config["print_report"]:
        print_metrics_report(metrics)

    print("Evaluation complete!")

# Add this to allow running the script directly
if __name__ == "__main__":
    main()