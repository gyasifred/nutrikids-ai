# -*- coding: utf-8 -*-
"""Llama3.1_(8B)-Alpaca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb
"""

#!/usr/bin/activate
from unsloth import FastLanguageModel
import torch
max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""<a name="Data"></a>
### Data Prep
We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.

**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).

**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!

If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb)

For text completions like novel writing, try this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb).
"""

import pandas as pd
from typing import List, Dict, Optional, Tuple
from datasets import Dataset
import random
import re
import json

class MalnutritionDataset:
    """Class to handle malnutrition dataset operations."""

    def __init__(self, data_path: str, note_col: str, label_col: str):
        """Initialize dataset from a CSV file.

        Args:
            data_path (str): Path to the CSV file containing the data
            note_col (str): Name of the text column in the CSV
            label_col (str): Name of the label column in the CSV
        """
        self.df = pd.read_csv(data_path)
        self.text_col = note_col
        self.label_col = label_col

    def prepare_training_data(self, prompt_builder, tokenizer) -> List[Dict[str, str]]:
        """Prepare data in the format required for training, following Alpaca-style formatting.

        Args:
            prompt_builder: An instance of MalnutritionPromptBuilder
            tokenizer: The tokenizer to use for adding EOS token

        Returns:
            List of dictionaries with formatted text for training
        """
        EOS_TOKEN = tokenizer.eos_token
        formatted_data = []

        for _, row in self.df.iterrows():
            # Generate prompt for each example
            prompt = prompt_builder.get_training_prompt(row[self.text_col])

            # Format label as "yes" or "no"
            label_text = "yes" if str(row[self.label_col]).lower() in ["1", "yes", "true"] else "no"

            # JSON format output as per the prompt builder's specifications
            output = json.dumps({"malnutrition": label_text,
                               "explanation": f"Based on the clinical evidence in the patient notes."})

            # Create formatted text with prompt and response, similar to Alpaca format
            formatted_text = f"{prompt}\n{output}{EOS_TOKEN}"

            formatted_data.append({
                "text": formatted_text
            })

        return formatted_data

    def to_huggingface_dataset(self, prompt_builder, tokenizer) -> Dataset:
        """Convert prepared data to a HuggingFace Dataset.

        Args:
            prompt_builder: An instance of MalnutritionPromptBuilder
            tokenizer: Tokenizer for adding EOS token

        Returns:
            HuggingFace Dataset ready for model training
        """
        formatted_data = self.prepare_training_data(prompt_builder, tokenizer)

        return Dataset.from_dict({
            "text": [item["text"] for item in formatted_data]
        })
    # ────────────────────────────────────────────────────────────────────────────────
# Enhanced clinical‑note cleaner
# ────────────────────────────────────────────────────────────────────────────────
def preprocess_text(text: str) -> str:
    """Light preprocessing to clean clinical text."""
    text = re.sub(r"\s+", " ", text)                    # collapse multiple spaces/newlines
    text = text.replace(" </s> ", "\n- ")               # convert separators to bullets
    text = re.sub(r"_date_|_lgnum_", "[REDACTED]", text)
    return text.strip()


# ────────────────────────────────────────────────────────────────────────────────
# Enhanced Prompt‑builder with improved prompt design
# ────────────────────────────────────────────────────────────────────────────────
class MalnutritionPromptBuilder:
    """Manage creation of malnutrition prompts for training or inference."""

    # ------------------------------------------------------------------ #
    # Initialise (optionally load CSV of few‑shot examples)
    # ------------------------------------------------------------------ #
    def __init__(self, examples_csv_path: Optional[str] = None):
        self.examples_csv_path = examples_csv_path
        self.examples_cache: Optional[pd.DataFrame] = None

        if examples_csv_path:
            try:
                self.examples_cache = pd.read_csv(examples_csv_path)
                print(f"[PromptBuilder] Loaded {len(self.examples_cache)} examples from {examples_csv_path}")
            except Exception as exc:  # noqa: BLE001
                print(f"[PromptBuilder] Error loading examples: {exc}")

    # ------------------------------------------------------------------ #
    # Public helpers
    # ------------------------------------------------------------------ #
    def get_training_prompt(self, patient_notes: str) -> str:
        """Return a zero‑shot prompt for supervised fine‑tuning."""
        return self._construct_prompt(patient_notes)

    def get_inference_prompt(
        self,
        patient_notes: str,
        note_col: str,
        label_col: str,
        *,
        num_examples: int = 0,
        specific_example_indices: Optional[List[int]] = None,
        balanced: bool = False,
    ) -> str:
        """
        Build an inference prompt with optional few‑shot examples.

        Parameters
        ----------
        patient_notes : str
            Note you want classified.
        note_col, label_col : str
            Column names in the CSV for the note text and label.
        num_examples : int
            Number of examples to prepend (0 → zero‑shot).
        specific_example_indices : list[int], optional
            Explicit rows to use as few‑shot examples.
        balanced : bool
            If True and `num_examples ≥ 2`, sample a 50/50 yes/no mix.
        """
        if num_examples == 0 or self.examples_cache is None:
            return self._construct_prompt(patient_notes)

        # balanced branch
        if balanced and num_examples >= 2:
            return self._get_balanced_prompt(
                patient_notes, note_col, label_col, num_examples=num_examples
            )

        # generic few‑shot sampling
        few_shot_examples: List[Dict[str, str]] = []
        if specific_example_indices:
            valid = [i for i in specific_example_indices if 0 <= i < len(self.examples_cache)]
            for idx in valid[: num_examples]:
                few_shot_examples.append(
                    {
                        "text": self.examples_cache.at[idx, note_col],
                        "label": self.examples_cache.at[idx, label_col],
                    }
                )
        else:
            chosen = random.sample(
                range(len(self.examples_cache)), k=min(num_examples, len(self.examples_cache))
            )
            for idx in chosen:
                few_shot_examples.append(
                    {
                        "text": self.examples_cache.at[idx, note_col],
                        "label": self.examples_cache.at[idx, label_col],
                    }
                )

        return self._construct_prompt(patient_notes, few_shot_examples)

    def get_balanced_inference_prompt(
        self,
        patient_notes: str,
        text_col: str,
        label_col: str,
        *,
        num_examples: int = 4,
    ) -> str:
        """Return a prompt with a balanced yes/no example mix."""
        return self._get_balanced_prompt(
            patient_notes, text_col, label_col, num_examples=num_examples
        )

    # ------------------------------------------------------------------ #
    # Few‑shot example formatter - enhanced for clarity
    # ------------------------------------------------------------------ #
    def _format_example(self, example: Dict[str, str]) -> str:
        """Return one example block that matches the JSON output spec."""
        notes = preprocess_text(example["text"])
        raw_label = str(example["label"]).lower()
        label = "yes" if raw_label in {"1", "yes", "true"} else "no"

        # Enhanced explanations based on label
        if label == "yes":
            explanations = [
                "Evidence of significant weight loss and reduced dietary intake.",
                "Z-scores below -2 SD with clinical signs of wasting.",
                "Poor nutritional intake with anthropometric measurements in malnutrition range.",
                "Clinical evidence of muscle wasting with documented inadequate intake.",
                "Height-for-age z-score below -3 SD indicating severe chronic malnutrition."
            ]
        else:
            explanations = [
                "Anthropometry within normal limits with adequate intake documented.",
                "No significant weight loss or reduced intake reported.",
                "All nutritional parameters within normal range.",
                "Growth and development on track with no clinical signs of malnutrition.",
                "Z-scores within normal limits and no reported feeding difficulties."
            ]

        explanation = random.choice(explanations)

        return (
            "Patient notes:\n"
            f"{notes}\n"
            "Output:\n"
            f'{{"malnutrition":"{label}","explanation":"{explanation}"}}\n'
        )

    # ------------------------------------------------------------------ #
    # ENHANCED PROMPT CONSTRUCTION - improved readability and effectiveness
    # ------------------------------------------------------------------ #
    def _construct_prompt(
        self,
        patient_notes: str,
        few_shot_examples: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        notes_clean = preprocess_text(patient_notes)

        header = (
            "You are a board‑certified clinical dietitian with expertise in pediatric malnutrition assessment."
        )

        task = (
            "# ASSESSMENT TASK\n"
            "Evaluate the patient notes to determine if there is clinical evidence of malnutrition.\n\n"
            "Classification guidelines:\n"
            "• \"yes\" - Patient meets at least MILD criteria for malnutrition\n"
            "• \"no\" - Patient does not meet ANY malnutrition criteria\n"
            "• IMPORTANT: If evidence is borderline or ambiguous, classify as \"no\"\n"
        )

        checklist = (
            "# MALNUTRITION DIAGNOSTIC CRITERIA\n\n"
            "1. **ANTHROPOMETRY**\n"
            "   ✓ Mild:     z-score -1.0 to -1.9 SD\n"
            "   ✓ Moderate: z-score -2.0 to -2.9 SD\n"
            "   ✓ Severe:   z-score ≤ -3.0 SD\n"
            "   ✓ Weight-for-height, BMI-for-age, or weight-for-age\n"
            "   ✓ Height-for-age z-score ≤ -3 SD indicates severe stunting\n"
            "   ✓ MUAC (Mid-Upper Arm Circumference) follows same cutoffs\n\n"

            "2. **WEIGHT LOSS**\n"
            "   ✓ Documented involuntary weight loss\n"
            "   ✓ Failure to gain expected weight/height in child\n"
            "   ✓ Declining percentile crossing on growth charts\n\n"

            "3. **REDUCED INTAKE/ABSORPTION**\n"
            "   ✓ Decreased appetite or food intake\n"
            "   ✓ Feeding difficulties or dysphagia\n"
            "   ✓ Restricted diet or food insecurity\n"
            "   ✓ Malabsorption conditions\n\n"

            "4. **CLINICAL ASSESSMENT**\n"
            "   ✓ Muscle wasting (temporal, extremities)\n"
            "   ✓ Subcutaneous fat loss\n"
            "   ✓ Edema (can mask weight loss)\n"
            "   ✓ Poor wound healing, skin/hair changes\n\n"

            "5. **COMPLICATING FACTORS**\n"
            "   ✓ Chronic illness/inflammation\n"
            "   ✓ Increased metabolic demand (infection, trauma)\n"
            "   ✓ Medication effects on intake/absorption\n"
            "   ✓ Psychosocial factors\n"
        )

        output_spec = (
            "# OUTPUT REQUIREMENTS\n"
            "Return a valid JSON object with these exact fields:\n"
            "```json\n"
            "{\n"
            '  "malnutrition": "yes" | "no",\n'
            '  "explanation": "<1-2 concise sentences citing specific evidence>"\n'
            "}\n"
            "```\n"
            "- Provide only the JSON object without additional text\n"
            "- Base assessment solely on evidence present in the notes\n"
            "- Include specific metrics/findings that support your conclusion\n"
            "- Do not show your detailed reasoning process\n"
        )

        # few‑shot examples block
        few_shot_block = ""
        if few_shot_examples:
            few_shot_block = (
                "# EXAMPLES\n"
                + "\n".join(self._format_example(ex) for ex in few_shot_examples)
                + "\n---\n"
            )

        # assemble prompt
        return (
            f"{header}\n\n{task}\n{checklist}\n{output_spec}\n"
            f"{few_shot_block}"
            "# PATIENT NOTES\n"
            f"{notes_clean}\n\n"
            "# ASSESSMENT"
        )

    # ------------------------------------------------------------------ #
    # Improved balanced few‑shot sampling helper
    # ------------------------------------------------------------------ #
    def _get_balanced_prompt(
        self,
        patient_notes: str,
        note_col: str,
        label_col: str,
        *,
        num_examples: int = 4,
    ) -> str:
        """Return a prompt with a balanced yes/no example mix."""
        if self.examples_cache is None:
            return self._construct_prompt(patient_notes)

        yes_mask = self.examples_cache[label_col].astype(str).str.lower().isin({"1", "yes", "true"})
        yes_df = self.examples_cache[yes_mask]
        no_df = self.examples_cache[~yes_mask]

        # Calculate how many examples to get from each class
        yes_count = len(yes_df)
        no_count = len(no_df)
        total_available = yes_count + no_count

        if total_available < num_examples:
            # Not enough examples, use all available
            few_shot_examples = []
            for i in range(yes_count):
                few_shot_examples.append({
                    "text": yes_df.iloc[i][note_col],
                    "label": yes_df.iloc[i][label_col]
                })
            for i in range(no_count):
                few_shot_examples.append({
                    "text": no_df.iloc[i][note_col],
                    "label": no_df.iloc[i][label_col]
                })
        else:
            # We have enough examples, try to balance
            half = num_examples // 2
            yes_needed = min(half, yes_count)
            no_needed = min(num_examples - yes_needed, no_count)

            # If we couldn't get enough of one class, get more from the other
            if yes_needed < half and no_count > no_needed:
                no_needed = min(num_examples - yes_needed, no_count)
            elif no_needed < (num_examples - half) and yes_count > yes_needed:
                yes_needed = min(num_examples - no_needed, yes_count)

            # Sample
            few_shot_examples = []
            if yes_needed > 0:
                yes_indices = random.sample(range(yes_count), k=yes_needed)
                for i in yes_indices:
                    few_shot_examples.append({
                        "text": yes_df.iloc[i][note_col],
                        "label": yes_df.iloc[i][label_col]
                    })

            if no_needed > 0:
                no_indices = random.sample(range(no_count), k=no_needed)
                for i in no_indices:
                    few_shot_examples.append({
                        "text": no_df.iloc[i][note_col],
                        "label": no_df.iloc[i][label_col]
                    })

        # Shuffle to avoid order bias
        random.shuffle(few_shot_examples)
        return self._construct_prompt(patient_notes, few_shot_examples)


# ────────────────────────────────────────────────────────────────────────────────
# Enhanced response parser with more robust extraction
# ────────────────────────────────────────────────────────────────────────────────
def extract_malnutrition_decision(response: str) -> Tuple[str, str]:
    """
    Extract the malnutrition decision (yes|no) and explanation from model output.
    Handles both JSON format and fallback extraction methods.

    Returns
    -------
    decision : str  ("yes", "no", or "unknown")
    explanation : str
    """
    # Clean the input string to handle markdown code blocks
    cleaned = response.strip()

    # Extract content from code blocks if present
    if "```" in cleaned:
        pattern = r'```(?:json)?\s*(.*?)\s*```'
        matches = re.findall(pattern, cleaned, re.DOTALL)
        if matches:
            # Use the first code block that looks like valid JSON
            for match in matches:
                try:
                    # Test if parseable as JSON
                    parsed_json = json.loads(match.strip())
                    cleaned = match.strip()
                    break
                except json.JSONDecodeError:
                    continue

    # Try direct JSON parsing first (most reliable method)
    try:
        parsed = json.loads(cleaned)
        # Look for either "malnutrition" key as specified in prompt
        if "malnutrition" in parsed:
            decision = str(parsed.get("malnutrition", "unknown")).lower()
            explanation = str(parsed.get("explanation", "")).strip()
            if decision in ["yes", "no"]:
                return decision, explanation
    except json.JSONDecodeError:
        pass

    # Fallback extraction methods for both JSON and non-JSON formats

    # 1. Extract malnutrition decision with improved patterns
    decision_patterns = [
        # JSON format patterns
        r'"malnutrition"\s*:\s*"(yes|no)"',  # Standard JSON format
        r'"malnutrition"\s*:\s*"([^"]+)"',   # Any string in JSON format

        # Non-JSON format patterns
        r'malnutrition\s*[=:]\s*(yes|no)',   # Key-value pair format
        r'malnutrition.*?(yes|no)',          # Loose format

        # Sentence-based patterns
        r"patient (does|doesn['']t) meet.*?malnutrition",  # Clinical statement
        r'(evidence|signs|indications) of malnutrition',    # Evidence statement
    ]

    decision = "unknown"
    for pattern in decision_patterns:
        decision_match = re.search(pattern, cleaned, flags=re.I)
        if decision_match:
            matched_text = decision_match.group(1).lower()
            # Map various positive/negative expressions to yes/no
            if matched_text in ["yes", "does", "evidence", "signs", "indications"]:
                decision = "yes"
                break
            elif matched_text in ["no", "doesn't", "doesn't", "doesnt"]:
                decision = "no"
                break
            elif matched_text in ["yes", "no"]:
                decision = matched_text
                break

    # 2. Extract explanation with improved patterns
    explanation = ""
    explanation_patterns = [
        # JSON format patterns
        r'"explanation"\s*:\s*"((?:[^"\\]|\\.|\\["\\])*)"',  # Quoted with possible escapes
        r'"explanation"\s*:\s*\'([^\']*)\'',                 # Single-quoted
        r'"explanation"\s*:\s*"([^"]*)"',                    # Double-quoted simple

        # Non-JSON formats
        r'explanation\s*[:=]\s*["\'](.*?)["\']',             # Quoted after key
        r'explanation\s*[:=]\s*([^",\s][^,}]*)',             # Unquoted after key

        # Context-based patterns
        r'malnutrition.*?because\s+(.*?)(?:\.|$)',           # After "because"
        r'(due to\s+.*?)(?:\.|$)',                           # "Due to" phrase
        r'evidence includes\s+(.*?)(?:\.|$)',                # Evidence statement
    ]

    for pattern in explanation_patterns:
        expl_match = re.search(pattern, cleaned, flags=re.I | re.DOTALL)
        if expl_match:
            # Get the first non-None group
            groups = expl_match.groups()
            explanation = next((g for g in groups if g is not None), "").strip()
            if explanation:
                break

    # As last resort, try to extract a relevant sentence if we have a decision but no explanation
    if decision != "unknown" and not explanation:
        sentences = re.split(r'[.!?]\s+', cleaned)
        for sentence in sentences:
            # Check if sentence contains malnutrition-related terms
            if re.search(r'malnutrition|nutritional|weight|growth|z-score|BMI', sentence, re.I):
                explanation = sentence.strip()
                break

    return decision, explanation

def formatting_prompts_func(examples):
    prompt_builder = MalnutritionPromptBuilder()
    texts = []

    for note, label in zip(examples[note_col], examples[label_col]):
        # Generate the prompt for each example
        prompt = prompt_builder.get_training_prompt(note)

        # Format the output as "yes" or "no"
        label_text = "yes" if str(label).lower() in ["1", "yes", "true"] else "no"

        # Create the full text with the response
        text = f"{prompt}\n{label_text}" + EOS_TOKEN
        texts.append(text)

    return {"text": texts}

# Create dataset and prepare data
dataset = MalnutritionDataset(data_path="data/notes_train.csv", note_col="txt", label_col="label")
prompt_builder = MalnutritionPromptBuilder()

# Prepare formatted data
formatted_data = dataset.prepare_training_data(prompt_builder, tokenizer)

# Convert to HuggingFace Dataset
from datasets import Dataset
hf_dataset = Dataset.from_dict({
    "text": [item["text"] for item in formatted_data]
})

"""<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!
"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        num_train_epochs = 5,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)

trainer_stats = trainer.train()

# @title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inference
Let's run the model! You can change the instruction and input - leave the output blank!

**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)**
"""

# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Continue the fibonnaci sequence.", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)

"""You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!

<a name="Save"></a>
### Saving, loading finetuned models
To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.

**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!
"""

model.save_pretrained("trained/LLm_final/final_model")
tokenizer.save_pretrained("trained/LLm_final/final_model")