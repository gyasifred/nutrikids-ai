# -*- coding: utf-8 -*-
"""non_instruction_finetuning_peft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bo1CbXY_kw6FgqkHApzp83fZE8X4UR7V

In this notebook, I will fine-tune `unsloth/Llama-3.2-1B-Instruct-bnb-4bit` (non-instruction) using LoRA, a parameter-efficient fine-tuning method, to detect `Pediatric Malnutrition` from clinical notes.

Begin with utility functions to process dataset
"""

from typing import List, Dict, Optional, Any
import pandas as pd
import random
import numpy as np
import torch
import os

class MalnutritionDataset:
    """Class to handle malnutrition dataset operations."""

    def __init__(self, data_path: str, note_col: str, label_col: str):
        """Initialize dataset from a CSV file.

        Args:
            data_path (str): Path to the CSV file containing the data
            note_col (str): Name of the text column in the CSV
            label_col (str): Name of the label column in the CSV
        """
        self.df = pd.read_csv(data_path)
        self.text = note_col
        self.label = label_col

    def prepare_training_data(self) -> List[Dict[str, str]]:
        """Prepare data in the format required for training.

        Args:
            None

        Returns:
            List of dictionaries with text and labels formatted for training
        """
        formatted_data = []
        for _, row in self.df.iterrows():
            # Generate prompt for each example
            note = row[self.text]

            formatted_data.append({
                "text": note,
                "labels": "yes" if str(row[self.label]).lower() in ["1", "yes", "true"] else "no"
            })

        return formatted_data

def set_seed(seed: int = 42):
    """Set random seed for reproducibility.

    Args:
        seed (int): Random seed value
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)


def is_bfloat16_supported():
    """Check if bfloat16 is supported by the current device.

    Returns:
        bool: True if bfloat16 is supported, False otherwise
    """
    return torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8

"""Check out our dataset"""

import torch
from datasets import Dataset
from transformers import BitsAndBytesConfig
from trl import SFTTrainer, SFTConfig
from unsloth import FastLanguageModel
import pandas as pd
import json

def get_quantization_config(config):
    """Define quantization configuration for the model based on arguments.

    Args:
        args: Command line arguments

    Returns:
        BitsAndBytesConfig: Quantization configuration
    """
    # Determine if we should use 8-bit or 4-bit quantization (but not both)
    if config['load_in_8bit']:
        return BitsAndBytesConfig(
            load_in_8bit=True,
            load_in_4bit=False,
            llm_int8_enable_fp32_cpu_offload=True
        )
    elif config['load_in_4bit']:
        # Determine compute dtype based on available hardware and args
        if config['force_bf16'] and is_bfloat16_supported():
            compute_dtype = torch.bfloat16
        else:
            compute_dtype = torch.float16

        return BitsAndBytesConfig(
            load_in_4bit=True,
            load_in_8bit=False,
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            llm_int8_enable_fp32_cpu_offload=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=True
        )
    else:
        # No quantization
        return None


def determine_model_precision(config):
    """Determine appropriate precision settings for model training.

    Args:
        args: Command line arguments

    Returns:
        tuple: (fp16, bf16) boolean flags
    """
    if config['force_fp16']:
        return True, False

    if config['force_bf16']:
        if is_bfloat16_supported():
            return False, True
        else:
            print("Warning: BF16 requested but not supported by hardware. Falling back to FP16.")
            return True, False

    # Auto-detect best precision
    if is_bfloat16_supported():
        return False, True
    else:
        return True, False


def load_model_and_tokenizer(config, quantization_config):
    """Load base model and tokenizer with appropriate settings."""
    print(f"Loading base model and tokenizer: {config['model_name']}")

    # Determine precision based on hardware and user preferences
    fp16, bf16 = determine_model_precision(config)
    dtype = torch.bfloat16 if bf16 else torch.float16

    try:
        # Ensure we're not using both 4-bit and 8-bit
        load_in_4bit = config['load_in_4bit'] and not config['load_in_8bit']
        load_in_8bit = config['load_in_8bit'] and not config['load_in_4bit']

        print(f"Loading model with settings: precision={'bf16' if bf16 else 'fp16'}, "
              f"load_in_4bit={load_in_4bit}, load_in_8bit={load_in_8bit}")

        # Set attention implementation based on flash attention flag
        attn_implementation = "flash_attention_2" if config['use_flash_attention'] else "eager"

        # Create kwargs for model loading
        model_kwargs = {
            "model_name": config['model_name'],
            "dtype": dtype,
            "device_map": "auto",
            "attn_implementation": attn_implementation,
        }

        # If quantization_config is provided, use it
        if quantization_config is not None:
            model_kwargs["quantization_config"] = quantization_config
        else:
            # Otherwise use the direct parameters
            model_kwargs["load_in_4bit"] = load_in_4bit
            model_kwargs["load_in_8bit"] = load_in_8bit

        # Load the model with the appropriate parameters
        base_model, tokenizer = FastLanguageModel.from_pretrained(**model_kwargs)

        print("Model and tokenizer loaded successfully.")
        return base_model, tokenizer, fp16, bf16
    except Exception as e:
        print(f"Error loading model: {e}")
        raise


def get_target_modules(args, model_name):
    """Determine appropriate target modules for LoRA based on model architecture.

    Args:
        args: Command line arguments
        model_name: Name of the model

    Returns:
        list: List of target module names
    """
    # If user specified target modules, use those
    if args['target_modules']:
        return args['target_modules'].split(',')

    # Default target modules based on model architecture
    model_name_lower = model_name.lower()

    if any(name in model_name_lower for name in ["llama", "mistral", "mixtral"]):
        return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif "phi" in model_name_lower:
        return ["q_proj", "k_proj", "v_proj", "o_proj"]
    elif "qwen" in model_name_lower:
        return ["q_proj", "k_proj", "v_proj", "o_proj", "w1", "w2"]
    elif "deepseek" in model_name_lower:
        return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

    # Default fallback
    return ["q_proj", "k_proj", "v_proj", "o_proj"]

def create_peft_model(base_model, config):
    """Create PEFT/LoRA model for fine-tuning with appropriate settings.

    Args:
        base_model: The base language model
        args: Command line arguments

    Returns:
        model: The PEFT model ready for training
    """
    print("Creating PEFT/LoRA model...")

    # Get appropriate target modules for this model architecture
    target_modules = get_target_modules(config, config['model_name'])
    print(f"Using target modules: {target_modules}")

    model = FastLanguageModel.get_peft_model(
        model=base_model,
        r=config['lora_r'],
        lora_alpha=config['lora_alpha'],
        lora_dropout=0,
        target_modules=target_modules,
        use_gradient_checkpointing=True,
        random_state=config['seed'],
        use_rslora=True,
        loftq_config=None
    )

    # Enable gradient checkpointing for efficient training
    model.gradient_checkpointing_enable()
    if hasattr(model, 'enable_input_require_grads'):
        model.enable_input_require_grads()

    return model


def get_sft_config(config, fp16, bf16):
    """Configure SFT training arguments.

    Args:
        args: Command line arguments
        fp16: Whether to use FP16 precision
        bf16: Whether to use BF16 precision

    Returns:
        SFTConfig: Configuration for SFT training
    """
    config_kwargs = {
        "per_device_train_batch_size": config['batch_size'],
        "gradient_accumulation_steps": config['gradient_accumulation'],
        "warmup_steps": 5,
        # "max_steps": args.max_steps,
        "learning_rate": config['learning_rate'],
        "fp16": fp16,
        "bf16": bf16,
        "logging_steps": 1,
        "optim": "adamw_8bit",
        "weight_decay": 0.01,
        "lr_scheduler_type": "linear",
        "seed": config['seed'],
        "output_dir": config['output_dir'],
        "report_to": config['report_to'],
        "save_strategy": "steps",
        "save_steps": 10,
        "max_seq_length": config['max_seq_length'],
        "dataset_num_proc": 4,
        "packing": False,
        "num_train_epochs": config['epochs']
    }

    # Add evaluation parameters only if validation data is provided
    if config['val_data'] is not None:
        config_kwargs.update({
            "eval_strategy": "steps",
            "eval_steps": 10,
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
        })

    print(f"Training with precision: fp16={fp16}, bf16={bf16}")
    return SFTConfig(**config_kwargs)


def prepare_datasets(train_data_path, val_data_path, tokenizer, note_col, label_col, max_seq_length):
    """Prepare training and validation datasets.

    Args:
        train_data_path (str): Path to training data CSV
        val_data_path (str): Path to validation data CSV
        prompt_builder: MalnutritionPromptBuilder instance
        tokenizer: Tokenizer for the model
        note_col (str): Name of the text column
        label_col (str): Name of the label column
        max_seq_length (int): Maximum sequence length for tokenization

    Returns:
        Tuple: (train_dataset, eval_dataset)
    """
    print("Preparing datasets...")

    # Load and prepare training data
    train_data = MalnutritionDataset(train_data_path, note_col, label_col)
    train_formatted = train_data.prepare_training_data()

    # Pre-tokenize the data to ensure consistent format
    def tokenize_function(examples):
        # Make sure 'text' field exists and is not empty
        if not examples.get('text'):
            return {"input_ids": [], "attention_mask": []}

        # Tokenize the examples
        tokenized = tokenizer(
            examples['text'],
            truncation=True,
            padding="max_length",
            max_length=max_seq_length,
            return_tensors=None,
        )
        # Add labels for supervised fine-tuning
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    # Convert to Dataset and tokenize
    train_dataset = Dataset.from_pandas(pd.DataFrame(train_formatted))
    train_tokenized = train_dataset.map(
        tokenize_function,
        batched=False,
        remove_columns=["text"] if "text" in train_dataset.column_names else [],
    )

    # Handle validation data if provided
    eval_tokenized = None
    if val_data_path is not None:
        val_data = MalnutritionDataset(val_data_path, note_col, label_col)
        val_formatted = val_data.prepare_training_data()
        eval_dataset = Dataset.from_pandas(pd.DataFrame(val_formatted))
        eval_tokenized = eval_dataset.map(
            tokenize_function,
            batched=False,
            remove_columns=["text"] if "text" in eval_dataset.column_names else [],
        )
        print(f"Prepared {len(train_tokenized)} training examples and {len(eval_tokenized)} validation examples")
    else:
        print(f"Prepared {len(train_tokenized)} training examples")

    return train_tokenized, eval_tokenized




config = {
  "model_name": "unsloth/Llama-3.2-1B-bnb-4bit",
  "train_data": "data/train_data.csv",
  "val_data":"data/val_data.csv",
  "text_column": "txt",
  "label_column": "label",
  # Output arguments
  "output_dir": "./meta",
  "model_output": "./llama3_2",

  # Training arguments
  "batch_size": 2,
  "gradient_accumulation": 4,
  "learning_rate": 2e-4,
  "max_seq_length": 2048,
  "epochs": 30,

  # LoRA parameters
  "lora_r": 8,
  "lora_alpha": 32,
  "target_modules": None,

  # Precision arguments
  "force_fp16": False,
  "force_bf16": False,

  # Miscellaneous
  "seed": 42,
  "use_flash_attention": False,
  "report_to": "none",
  "load_in_8bit": False,
  "load_in_4bit": True
}

# Ensure mutual exclusivity of precision settings
if config["load_in_8bit"]:
    config["load_in_4bit"] = False

# Set seed for reproducibility
set_seed(config['seed'])

# Create output directories
os.makedirs(config['output_dir'], exist_ok=True)
os.makedirs(config['model_output'], exist_ok=True)

# Save configuration
with open(os.path.join(config['output_dir'], "config.json"), "w") as f:
    json.dump(config, f, indent=4)

# Get quantization config
quantization_config = get_quantization_config(config)

# Load model and tokenizer with precision detection
base_model, tokenizer, fp16, bf16 = load_model_and_tokenizer(
    config, quantization_config
)

# Load and prepare datasets with tokenization
train_dataset, eval_dataset = prepare_datasets(
    config['train_data'],
    config['val_data'],
    tokenizer,
    config['text_column'],
    config['label_column'],
    config['max_seq_length']
)

# Create PEFT/LoRA model
model = create_peft_model(base_model, config)

# Get SFT config with correct precision settings
sft_config = get_sft_config(config, fp16, bf16)

# Initialize SFT trainer
trainer_kwargs = {
    "model": model,
    "processing_class": tokenizer,
    "train_dataset": train_dataset,
    "args": sft_config,
}

if eval_dataset is not None:
    trainer_kwargs["eval_dataset"] = eval_dataset

trainer = SFTTrainer(**trainer_kwargs)

import os
os.environ['UNSLOTH_RETURN_LOGITS'] = '1'

# Train the model
print(f"Starting training with {len(train_dataset)} examples for {config['epochs']} epoch(s)...")
trainer.train()

# Save the final model
print(f"Training completed. Saving model to {config['model_output']}")
trainer.tokenizer.save_pretrained(config['model_output'])
trainer.model.save_pretrained(config['model_output'])
print("Fine-tuning complete!")
