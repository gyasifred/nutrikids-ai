# -*- coding: utf-8 -*-
"""self_correction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlO3wCsGlAudy0s6qFaWDuAkuOGppYRF
"""

import re
import os
os.environ['UNSLOTH_RETURN_LOGITS'] = '1'
import pandas as pd
import torch
import json
import random
from datasets import Dataset
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from typing import List, Dict, Tuple, Optional
import numpy as np


def extract_malnutrition_result(text):
    """Simplified extraction focusing on the exact format required by the prompt"""
    if not text or not isinstance(text, str):
        return "no"
    
    # Convert to lowercase for matching
    text_lower = text.strip().lower()
    
    # Look for the exact format: malnutrition=yes/no at the end
    lines = text_lower.split('\n')
    
    # Check the last few lines for the exact format
    for line in reversed(lines[-5:]):
        line = line.strip()
        
        # Exact format match
        if re.match(r'^\s*malnutrition\s*=\s*(yes|no)\s*$', line):
            match = re.search(r'malnutrition\s*=\s*(yes|no)', line)
            if match:
                return match.group(1)
    
    # Check the entire text for the pattern if not found at the end
    pattern = r'malnutrition\s*=\s*(yes|no)'
    match = re.search(pattern, text_lower)
    if match:
        return match.group(1)
    
    # If no clear format found, look for strong clinical conclusions
    if any(phrase in text_lower for phrase in [
        "malnutrition is present", "has malnutrition", "diagnosed with malnutrition",
        "meets malnutrition criteria", "consistent with malnutrition"
    ]):
        return "yes"
    
    if any(phrase in text_lower for phrase in [
        "no malnutrition", "not malnourished", "does not have malnutrition",
        "no evidence of malnutrition", "normal nutritional status"
    ]):
        return "no"
    
    # Default to "no" if unclear
    return "no"


# Enhanced instruction for pediatric malnutrition diagnosis using small LLMs
instruction = """Read the patient's notes and determine if the patient is likely to have malnutrition according to the criteria below.

====== STEP-BY-STEP ANALYSIS PROCESS ======

STEP 1: EXTRACT KEY DATA FROM NOTES
Age: ___
Weight-for-height z-score: ___
BMI-for-age z-score: ___  
Height-for-age z-score: ___
MUAC z-score: ___
Weight change: ___% over ___ timeframe
Nutrient intake: ___% of estimated needs

STEP 2: DETERMINE APPROACH
□ Single data point available → Use Table 1
□ Multiple data points available → Use Table 2  
□ Both available → Use both, choose more severe classification

STEP 3: APPLY QUICK SCREENING
🔴 IMMEDIATE SEVERE: Any z-score ≤ -3.0 OR weight loss ≥10% OR intake <25%
🟠 IMMEDIATE MODERATE: Any z-score -2.0 to -2.9 OR weight loss 7.5-10% OR intake 26-50%
🟡 IMMEDIATE MILD: Any z-score -1.0 to -1.9 OR weight loss 5-7.5% OR intake 51-75%

====== MALNUTRITION CLASSIFICATION CRITERIA ======

Mild malnutrition related to undernutrition is usually the result of an acute event, either due to economic circumstances or acute illness, and presents with unintentional weight loss or weight gain velocity less than expected. Moderate malnutrition related to undernutrition occurs due to undernutrition of a significant duration that results in weight-for-length/height values or BMI-for-age values that are below the normal range. Severe malnutrition related to undernutrition occurs as a result of prolonged undernutrition and is most frequently quantified by declines in rates of linear growth that result in stunting.

You should use z scores (also called z for short) for weight-for-height/length, BMI-for-age, length/height-for-age or MUAC criteria. When a child has only one data point in the records (single z score present) use the table below:

Table 1. Single data point present.
Mild Malnutrition
Weight-for-height: −1 to −1.9 z score
BMI-for-age: −1 to −1.9 z score
Length/height-for-age: No Data
Mid–upper arm circumference: Greater than or equal to −1 to −1.9 z score	

Moderate Malnutrition	
Weight-for-height: −2 to −2.9 z score
BMI-for-age: −2 to −2.9 z score
Length/height-for-age: No Data
Mid–upper arm circumference: Greater than or equal to −2 to −2.9 z score	

Severe Malnutrition
Weight-for-height: −3 or greater z score
BMI-for-age: −3 or greater z score
Length/height-for-age: −3 z score
Mid–upper arm circumference: Greater than or equal to −3 z score

When the child has 2 or more data points (multiple z scores over time) use this table:
Table 2. Multiple data points available.
Mild Malnutrition
Weight gain velocity (<2 years of age): Less than 75% of the norm for expected weight gain
Weight loss (2–20 years of age): 5% usual body weight
Deceleration in weight for length/height: Decline of 1 z score
Inadequate nutrient intake: 51%−75% estimated energy/protein need

Moderate Malnutrition	
Weight gain velocity (<2 years of age): Less than 50% of the norm for expected weight gain
Weight loss (2–20 years of age): 7.5% usual body weight
Deceleration in weight for length/height: Decline of 2 z score
Inadequate nutrient intake: 26%−50% estimated energy/protein need

Severe Malnutrition
Weight gain velocity (<2 years of age): Less than 25% of the normb for expected weight gain
Weight loss (2–20 years of age): 10% usual body weight
Deceleration in weight for length/height: Decline of 3 z score
Inadequate nutrient intake: less than 25% estimated energy/protein need

====== CRITICAL DECISION RULES ======
⚠️ When multiple criteria conflict, choose the MORE SEVERE classification
⚠️ Height-for-age z-score indicates stunting but requires ≤-3.0 for severe malnutrition diagnosis
⚠️ A single normal measurement does NOT rule out malnutrition if other criteria are abnormal
⚠️ For infants <2 years, prioritize weight gain velocity over static measurements when both available
⚠️ ANY z-score ≤ -3.0 automatically indicates severe malnutrition
⚠️ Weight loss percentages: 5-7.5% = mild, 7.5-10% = moderate, ≥10% = severe

====== Z-SCORE INTERPRETATION HELPER ======
🟢 Normal: z-score > -1.0 (example: -0.5, -0.9)
🟡 Mild: z-score -1.0 to -1.9 (example: -1.2, -1.8)
🟠 Moderate: z-score -2.0 to -2.9 (example: -2.1, -2.7)
🔴 Severe: z-score ≤ -3.0 (example: -3.0, -3.5)

====== OUTPUT FORMAT INSTRUCTIONS ======

Provide your analysis in two parts:

PART 1: ANALYSIS
Use this template: "I extracted the following data: [list key values found in notes]. I used [single/multiple] data point criteria from [Table 1/Table 2/both tables]. The key finding is [specific measurement that determines classification]. This indicates [mild/moderate/severe/no] malnutrition because [brief clinical reasoning]."

PART 2: FINAL CLASSIFICATION
- As the VERY LAST LINE of your response, provide EXACTLY one of these two classifications:
  malnutrition=yes
  malnutrition=no

CRITICAL FORMATTING RULES:
1. Do NOT change the spelling of "malnutrition" - it must be spelled EXACTLY as shown
2. Use EXACTLY the format shown: malnutrition=yes or malnutrition=no
3. No spaces around the equals sign
4. No periods, commas or other punctuation
5. The classification MUST be on its own line at the very end of your response
6. Do NOT explain your final classification - just state it
7. Even if uncertain, you MUST choose either yes or no - do not say "maybe"

====== EXAMPLE RESPONSES ======

EXAMPLE 1:
I extracted the following data: weight loss 6% over 3 months, nutrient intake 60% of estimated needs. I used multiple data point criteria from Table 2. The key finding is weight loss of 6% which exceeds the 5% threshold for mild malnutrition. This indicates mild malnutrition because the patient meets both weight loss and inadequate intake criteria from Table 2.

malnutrition=yes

EXAMPLE 2: 
I extracted the following data: BMI-for-age z-score -0.8, weight-for-height z-score -0.5. I used single data point criteria from Table 1. The key finding is both z-scores are above -1.0 threshold. This indicates no malnutrition because all measurements are within normal range.

malnutrition=no

EXAMPLE 3:
I extracted the following data: weight-for-height z-score -2.3, BMI-for-age z-score -2.1, nutrient intake 40% of needs. I used both single and multiple data point criteria from Tables 1 and 2. The key finding is z-scores in the -2.0 to -2.9 range indicating moderate malnutrition. This indicates moderate malnutrition because both anthropometric measurements and nutritional intake criteria are met.

malnutrition=yes

====== EDGE CASE EXAMPLES ======

EDGE CASE 1 - Conflicting Data:
Input: 18-month-old with weight-for-height z-score -1.2 but weight gain velocity only 40% of expected over past 3 months.

Response: I extracted the following data: weight-for-height z-score -1.2, weight gain velocity 40% of expected. I used both single and multiple data point criteria. The key finding is weight gain velocity <50% of expected, which indicates moderate malnutrition from Table 2, overriding the mild classification from the single z-score. This indicates moderate malnutrition because longitudinal growth failure takes precedence over single measurements in infants.

malnutrition=yes

EDGE CASE 2 - Borderline Values:
Input: 5-year-old with BMI-for-age z-score exactly -2.0 and no other data available.

Response: I extracted the following data: BMI-for-age z-score -2.0. I used single data point criteria from Table 1. The key finding is z-score of exactly -2.0 which falls at the threshold for moderate malnutrition. This indicates moderate malnutrition because -2.0 meets the criteria for the -2 to -2.9 range.

malnutrition=yes

EDGE CASE 3 - Height-for-age Only:
Input: 3-year-old with height-for-age z-score -2.5, no weight data available.

Response: I extracted the following data: height-for-age z-score -2.5. I used single data point criteria from Table 1. The key finding is height-for-age z-score does not indicate acute malnutrition unless ≤-3.0. This indicates no malnutrition because height-for-age z-score of -2.5 indicates stunting but does not meet criteria for malnutrition diagnosis by itself.

malnutrition=no"""

# Enhanced self-correction instruction template
self_correction_instruction = """Review your previous analysis and determine if it was correct. If you find any errors in your reasoning or classification, provide a corrected analysis.

SYSTEMATIC REVIEW PROCESS:
1. Clinical Data Verification:
   - Did you correctly identify all relevant z-scores and their values?
   - Did you properly categorize single vs. multiple data point scenarios?
   - Did you miss any weight loss percentages or intake information?

2. Criteria Application Check:
   - Did you use the correct table (Table 1 vs Table 2)?
   - Did you apply the threshold values accurately?
   - Did you consider all relevant clinical indicators?

3. Logical Consistency:
   - Is your final classification consistent with your analysis?
   - Did you properly weigh contradictory evidence?
   - Are there any gaps in your reasoning?

4. Format Compliance:
   - Did you end with the exact format: malnutrition=yes or malnutrition=no?
   - Is the classification on its own line?

If your previous analysis was correct, confirm it briefly. If there were errors, provide the corrected analysis and classification using the same format requirements."""


class EnhancedSelfCorrectionTrainer:
    def __init__(self, model_name: str, max_length: int = 8192):
        self.model_name = model_name
        self.max_length = max_length
        self.model = None
        self.tokenizer = None
        self.extraction_stats = {"successful": 0, "failed": 0, "ambiguous": 0}

    def load_model(self):
        """Load the base model and tokenizer with enhanced configuration"""
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.model_name,
            max_seq_length=self.max_length,
            dtype=None,
            load_in_4bit=True
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.tokenizer.model_max_length = self.max_length
        self.tokenizer.truncation_side = "right"

    def generate_initial_predictions(self, notes: List[str], batch_size: int = 4) -> List[Dict]:
        """Generate initial predictions with enhanced error handling"""
        # Check if model is properly loaded
        if self.model is None:
            raise ValueError("Model is not loaded. Call load_model() first or ensure model is properly initialized.")
        
        predictions = []
    
        for i in range(0, len(notes), batch_size):
            batch_notes = notes[i:i+batch_size]
            batch_predictions = []
    
            for note in batch_notes:
                try:
                    prompt = self.build_prompt(note)
    
                    # Generate prediction with controlled parameters
                    inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.max_length)
                    
                    # Move inputs to the same device as model
                    if torch.cuda.is_available() and next(self.model.parameters()).is_cuda:
                        inputs = {k: v.cuda() for k, v in inputs.items()}
    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=512,
                            temperature=0.3,  # Lower temperature for more consistent outputs
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id,
                            repetition_penalty=1.1,
                            top_p=0.9
                        )
    
                    # Decode the generated response
                    generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    response = generated_text[len(prompt):].strip()
    
                    classification = self.extract_classification(response)
    
                    batch_predictions.append({
                        'note': note,
                        'prompt': prompt,
                        'initial_response': response,
                        'initial_classification': classification,
                        'extraction_confidence': self.assess_extraction_confidence(response, classification)
                    })
    
                except Exception as e:
                    print(f"Error generating prediction: {e}")
                    batch_predictions.append({
                        'note': note,
                        'prompt': self.build_prompt(note),
                        'initial_response': f"Error generating response: {e}",
                        'initial_classification': 'no',
                        'extraction_confidence': 'low'
                    })
    
            predictions.extend(batch_predictions)
    
        return predictions

    def extract_classification(self, response: str) -> str:
        """Use the enhanced extraction function"""
        result = extract_malnutrition_result(response)

        # Update statistics
        if result in ['yes', 'no']:
            if 'malnutrition=' in response.lower():
                self.extraction_stats["successful"] += 1
            else:
                self.extraction_stats["ambiguous"] += 1
        else:
            self.extraction_stats["failed"] += 1

        return result

    def assess_extraction_confidence(self, response: str, classification: str) -> str:
        """Assess confidence in extraction result"""
        response_lower = response.lower()

        # High confidence: exact format found
        if re.search(r'malnutrition\s*=\s*(yes|no)', response_lower):
            return 'high'

        # Medium confidence: clear clinical conclusion
        positive_indicators = ['malnutrition is present', 'has malnutrition', 'diagnosed with malnutrition']
        negative_indicators = ['no malnutrition', 'not malnourished', 'normal nutritional status']

        if any(indicator in response_lower for indicator in positive_indicators + negative_indicators):
            return 'medium'

        # Low confidence: inference based
        return 'low'

    def build_prompt(self, note: str) -> str:
        """Build the standard prompt with enhanced formatting"""
        return f"### Instruction:\n{instruction}\n\n### Input:\n{note.strip()}\n\n### Response:"

    def build_correction_prompt(self, note: str, initial_response: str, true_label: str = None) -> str:
        """Build enhanced prompt for self-correction"""
        correction_prompt = f"""### Instruction:
{instruction}

### Input:
{note.strip()}

### Previous Response:
{initial_response}

### Self-Correction Task:
{self_correction_instruction}

### Corrected Response:"""
        return correction_prompt

    def create_self_correction_examples(self, eval_data: pd.DataFrame,
                                 initial_predictions: List[Dict]) -> List[Dict]:
        """Create self-correction training examples using validation data"""
        correction_examples = []

        for i, (_, row) in enumerate(eval_data.iterrows()):
            if i >= len(initial_predictions):
                break

            pred_data = initial_predictions[i]
            true_label = str(row['label']).strip().lower()
            true_label = 'yes' if true_label in ['yes', '1', '1.0', 'true', 'positive'] else 'no'

            initial_pred = pred_data['initial_classification']

            # Create correction for all examples (both correct and incorrect)
            correction_type = 'error_correction' if initial_pred != true_label else 'reinforcement'

            corrected_response = self.create_corrected_response(
                pred_data['note'], pred_data['initial_response'], true_label, correction_type
            )

            correction_prompt = self.build_correction_prompt(
                pred_data['note'], pred_data['initial_response']
            )

            correction_examples.append({
                'prompt': correction_prompt,
                'response': corrected_response,
                'note': pred_data['note'],
                'initial_pred': initial_pred,
                'true_label': true_label,
                'correction_type': correction_type
            })

        return correction_examples

    def create_corrected_response(self, note: str, initial_response: str, true_label: str, correction_type: str) -> str:
        """Create enhanced corrected responses based on correction type"""

        if correction_type == 'error_correction':
            if true_label == 'yes':
                correction = f"""Upon systematic review of my previous analysis, I identified several critical errors:

My previous response failed to properly recognize key malnutrition indicators present in the clinical data. Let me provide a corrected analysis:

{self.generate_clinical_analysis(note, true_label)}

The evidence clearly supports malnutrition classification based on the established clinical criteria.

malnutrition=yes"""
            else:
                correction = f"""Upon systematic review of my previous analysis, I found errors in my clinical interpretation:

My previous response incorrectly identified malnutrition indicators. Let me provide a corrected analysis:

{self.generate_clinical_analysis(note, true_label)}

The clinical data does not meet the criteria for malnutrition diagnosis.

malnutrition=no"""

        elif correction_type == 'confidence_reinforcement':
            correction = f"""Upon review of my previous analysis, while my classification was correct, I can provide a more confident and thorough explanation:

{self.generate_clinical_analysis(note, true_label)}

This reinforces my original classification with greater clinical clarity.

malnutrition={true_label}"""

        else:  # stability_reinforcement or standard_reinforcement
            correction = f"""Upon systematic review of my previous analysis, I confirm that my reasoning and classification were accurate:

{self.generate_clinical_analysis(note, true_label)}

The analysis properly applied the malnutrition classification criteria and the evidence supports this conclusion.

malnutrition={true_label}"""

        return correction

    def generate_clinical_analysis(self, note: str, true_label: str) -> str:
        """Generate clinically-informed analysis based on note content"""
        # Extract key clinical indicators from the note
        indicators = self.extract_comprehensive_indicators(note)

        if true_label == 'yes':
            analysis_parts = []

            if indicators['z_scores']:
                z_scores = [float(z) for z in indicators['z_scores'] if self.is_valid_z_score(z)]
                significant_z = [z for z in z_scores if z <= -1.0]
                if significant_z:
                    analysis_parts.append(f"The documented z-scores {significant_z} fall below normal thresholds, indicating malnutrition severity according to clinical criteria.")

            if indicators['weight_loss']:
                analysis_parts.append(f"Weight loss indicators ({indicators['weight_loss']}) exceed the clinical thresholds for malnutrition classification.")

            if indicators['intake_deficiency']:
                analysis_parts.append(f"Inadequate nutrient intake documented ({indicators['intake_deficiency']}) falls within malnutrition criteria ranges.")

            if not analysis_parts:
                analysis_parts.append("Multiple clinical indicators in the documentation support malnutrition classification when properly evaluated against established criteria.")

            return " ".join(analysis_parts)

        else:  # true_label == 'no'
            analysis_parts = []

            if indicators['z_scores']:
                z_scores = [float(z) for z in indicators['z_scores'] if self.is_valid_z_score(z)]
                normal_z = [z for z in z_scores if z > -1.0]
                if normal_z:
                    analysis_parts.append(f"The documented z-scores {normal_z} fall within normal ranges (above -1.0 threshold).")

            analysis_parts.append("Clinical documentation shows growth parameters, nutritional intake, and physical findings within normal limits.")
            analysis_parts.append("No indicators meet the established criteria for malnutrition classification.")

            return " ".join(analysis_parts)

    def extract_comprehensive_indicators(self, note: str) -> Dict:
        """Extract comprehensive clinical indicators from note"""
        indicators = {
            'z_scores': [],
            'weight_loss': [],
            'intake_deficiency': [],
            'physical_findings': [],
            'age_info': None
        }

        note_lower = note.lower()

        # Extract z-scores with improved patterns
        z_patterns = [
            r'z[-\s]?score[:\s]*[-+]?(\d+\.?\d*)',
            r'weight[-\s]for[-\s]height[:\s]*z[-\s]?score[:\s]*[-+]?(\d+\.?\d*)',
            r'bmi[-\s]for[-\s]age[:\s]*z[-\s]?score[:\s]*[-+]?(\d+\.?\d*)',
            r'z[-\s]?score[:\s]*of[-\s]*[-+]?(\d+\.?\d*)'
        ]

        for pattern in z_patterns:
            matches = re.findall(pattern, note_lower)
            indicators['z_scores'].extend(matches)

        # Extract weight loss information
        weight_patterns = [
            r'weight\s+loss[:\s]*(\d+\.?\d*)%?',
            r'lost[:\s]*(\d+\.?\d*)%?\s*weight',
            r'(\d+\.?\d*)%\s*weight\s*loss'
        ]

        for pattern in weight_patterns:
            matches = re.findall(pattern, note_lower)
            indicators['weight_loss'].extend(matches)

       # Extract intake information
        intake_patterns = [
            r'intake[:\s]*(\d+\.?\d*)%',                    # "intake: 75%" or "intake 80.5%"
            r'(\d+\.?\d*)%\s*of\s*estimated\s*needs',      # "85% of estimated needs"
            r'eating[:\s]*(\d+\.?\d*)%',                    # "eating: 60%" or "eating 70.2%"
            r'consumed[:\s]*(\d+\.?\d*)%',                  # "consumed: 90%"
            r'(\d+\.?\d*)%\s*intake',                       # "75% intake"
            r'(\d+\.?\d*)%\s*consumption',                  # "80% consumption"
            r'oral\s*intake[:\s]*(\d+\.?\d*)%',            # "oral intake: 85%"
            r'food\s*intake[:\s]*(\d+\.?\d*)%',            # "food intake: 70%"
        ]

        for pattern in intake_patterns:
            matches = re.findall(pattern, note_lower)
            indicators['intake_deficiency'].extend(matches)

        # Extract age information
        age_patterns = [
            r'(\d+)[-\s]?(month|year)s?[-\s]?old',
            r'age[:\s]*(\d+)[-\s]?(month|year)s?',
            r'(\d+)[-\s]?(mo|yr)s?[-\s]?old'
        ]

        for pattern in age_patterns:
            matches = re.findall(pattern, note_lower)
            if matches:
                indicators['age_info'] = matches[0]
                break

        # Extract physical findings
        physical_patterns = [
            r'wasting', r'decreased subcutaneous fat', r'muscle wasting',
            r'poor muscle tone', r'appropriate subcutaneous fat', r'normal muscle tone'
        ]

        for pattern in physical_patterns:
            if pattern in note_lower:
                indicators['physical_findings'].append(pattern)

        return indicators

    def is_valid_z_score(self, z_str: str) -> bool:
        """Check if z-score string is valid"""
        try:
            z_val = float(z_str)
            return -10.0 <= z_val <= 10.0  # Reasonable z-score range
        except (ValueError, TypeError):
            return False

    def create_combined_dataset(self, original_data: List[Dict],
                            correction_data: List[Dict]) -> List[Dict]:
        """Combine original training data with correction examples"""
        combined_data = []
        
        # Add all original training data
        combined_data.extend(original_data)
        
        # Add all correction examples
        combined_data.extend(correction_data)
        
        # Shuffle the combined dataset
        random.shuffle(combined_data)
        
        return combined_data


def preprocess_clinical_note(note_text):
    """Enhanced preprocessing that keeps clinical data intact while removing problematic patterns."""
    if not note_text:
        return ""

    # Preserve clinical abbreviations and numbers while removing artifacts
    note_text = re.sub(r'[*_\-=+#~^`]{2,}', ' ', note_text)  # Remove repeating special chars
    note_text = re.sub(r'<[^>]+>', ' ', note_text)           # Remove HTML/XML tags
    note_text = re.sub(r'\s{2,}', ' ', note_text)            # Normalize whitespace

    # Handle special tokens without affecting clinical content
    note_text = note_text.replace('</s>', '\n\n')
    special_tokens = ['<s>', '<pad>', '</pad>', '<eos>', '<bos>']
    for token in special_tokens:
        note_text = note_text.replace(token, ' ')

    return note_text.strip()

def train_malnutrition_model_with_enhanced_self_correction(
    data_path: str,
    model_name: str = "unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit",
    output_dir: str = "./malnutrition_models",
    max_length: int = 32000,
    num_epochs: int = 3,
    batch_size: int = 4,
    self_correction_epochs: int = 2
):
    """
    Train a malnutrition classification model with enhanced self-correction training
    """

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    # Load and prepare dataset
    try:
        df = pd.read_csv(data_path, usecols=['DEID','txt','label'])
        print(f"Loaded dataset with {len(df)} examples")
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return None

    # Preprocess data
    df["txt"] = df["txt"].fillna("").apply(preprocess_clinical_note)
    df["label"] = df["label"].fillna("no")

    # Create train/validation split
    dataset = Dataset.from_pandas(df)
    dataset_dict = dataset.train_test_split(test_size=0.1, seed=3407)
    train_dataset = dataset_dict["train"]
    eval_dataset = dataset_dict["test"]

    print(f"Training set: {len(train_dataset)} examples")
    print(f"Validation set: {len(eval_dataset)} examples")

    # Initialize enhanced self-correction trainer
    sc_trainer = EnhancedSelfCorrectionTrainer(model_name, max_length)
    sc_trainer.load_model()

    # Prepare model for training (Phase 1)
    phase1_model = FastLanguageModel.get_peft_model(
        sc_trainer.model,
        r=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_alpha=32,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
        use_rslora=False,
        loftq_config=None,
    )

    # PHASE 1: Initial training on original data
    print("=" * 60)
    print("PHASE 1: Initial Training")
    print("=" * 60)

    def format_original_example(example):
        prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{example['txt']}\n\n### Response:"
        label = str(example.get("label", "no")).strip().lower()
        if label not in ["yes", "no"]:
            positive_indicators = ["1", "1.0", "true", "positive", "y"]
            label = "yes" if any(str(label) == ind for ind in positive_indicators) else "no"
        response = f"malnutrition={label}"
        return {"text": f"{prompt}\n{response}{sc_trainer.tokenizer.eos_token}"}

    # Process original training data
    train_formatted = train_dataset.map(format_original_example)

    # Initial training
    training_args = TrainingArguments(
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=1,
        warmup_ratio=0.1,
        num_train_epochs=num_epochs,
        learning_rate=2e-5,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.1,
        lr_scheduler_type="linear",
        seed=3407,
        report_to="none",
        output_dir=os.path.join(output_dir, "phase1"),
        dataloader_drop_last=True,
        remove_unused_columns=False
    )

    trainer = SFTTrainer(
        model=phase1_model,
        train_dataset=train_formatted,
        processing_class=sc_trainer.tokenizer,
        max_seq_length=max_length,
        dataset_text_field="text",
        packing=True,
        args=training_args,
    )

    print("Starting initial training...")
    trainer.train()

    # PHASE 2: Generate predictions for self-correction using validation data
    print("=" * 60)
    print("PHASE 2: Generating Self-Correction Data")
    print("=" * 60)
    
    # Create inference trainer with the Phase 1 trained model
    inference_trainer = EnhancedSelfCorrectionTrainer(model_name, max_length)
    inference_trainer.model = phase1_model
    inference_trainer.tokenizer = sc_trainer.tokenizer
    # Set model to inference mode
    FastLanguageModel.for_inference(inference_trainer.model)
    
    # Use validation data for predictions
    print("Using validation data for self-correction...")
    eval_notes = [example['txt'] for example in eval_dataset]
    eval_df = pd.DataFrame(eval_dataset)
    
    print(f"Generating predictions on {len(eval_notes)} validation examples...")
    
    # Generate initial predictions on validation data
    initial_predictions = []
    batch_size_inference = 1  # Use smaller batch size for inference
    
    for i in range(0, len(eval_notes), batch_size_inference):
        batch_notes = eval_notes[i:i+batch_size_inference]
        try:
            batch_predictions = inference_trainer.generate_initial_predictions(
                batch_notes, batch_size=batch_size_inference
            )
            initial_predictions.extend(batch_predictions)
            print(f"Processed {len(initial_predictions)}/{len(eval_notes)} predictions")
        except Exception as e:
            print(f"Error in batch {i//batch_size_inference}: {e}")
            # Add fallback predictions for failed batches
            for note in batch_notes:
                initial_predictions.append({
                    'note': note,
                    'prompt': inference_trainer.build_prompt(note),
                    'initial_response': "Unable to generate prediction due to error.",
                    'initial_classification': 'no',
                    'extraction_confidence': 'low'
                })
    
    # Create self-correction examples using validation data
    print("Creating self-correction examples...")
    correction_examples = inference_trainer.create_self_correction_examples(
        eval_df, initial_predictions
    )
    
    print(f"Created {len(correction_examples)} self-correction examples")
    print(f"Extraction statistics: {inference_trainer.extraction_stats}")

    # PHASE 3: Self-correction training continuing from Phase 1 model
    print("=" * 60)
    print("PHASE 3: Enhanced Self-Correction Training")
    print("=" * 60)

    # Clean up the trainer but keep the trained model
    del trainer
    torch.cuda.empty_cache()

    # Continue from Phase 1 trained model
    print("Continuing training from Phase 1 trained model...")
    
    # Set the Phase 1 model back to training mode
    FastLanguageModel.for_training(phase1_model)
    
    # Use the Phase 1 trained model for Phase 3
    correction_model = phase1_model

    # Format correction examples
    def format_correction_example(example):
        return {"text": f"{example['prompt']}\n{example['response']}{sc_trainer.tokenizer.eos_token}"}

    correction_dataset = Dataset.from_list(correction_examples)
    correction_formatted = correction_dataset.map(format_correction_example)

    # Create dummy trainer instance to use the method
    dummy_trainer = EnhancedSelfCorrectionTrainer(model_name, max_length)
    
    # Combine original training data with correction data
    original_examples = [{"text": ex["text"]} for ex in train_formatted]
    correction_examples_formatted = [{"text": ex["text"]} for ex in correction_formatted]

    combined_examples = dummy_trainer.create_combined_dataset(
        original_examples, correction_examples_formatted
    )

    combined_dataset = Dataset.from_list(combined_examples)

    print(f"Combined dataset size: {len(combined_dataset)} examples")
    print(f"Original training examples: {len(original_examples)}")
    print(f"Correction examples: {len(correction_examples_formatted)}")

    # Self-correction training with adjusted parameters
    correction_training_args = TrainingArguments(
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=2,
        warmup_ratio=0.05,
        num_train_epochs=self_correction_epochs,
        learning_rate=1e-5,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=5,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        seed=3407,
        report_to="none",
        output_dir=os.path.join(output_dir, "phase3"),
        dataloader_drop_last=True,
        remove_unused_columns=False,
        save_strategy="steps",
        save_steps=50,
        eval_strategy="no"
    )

    correction_trainer = SFTTrainer(
        model=correction_model,
        train_dataset=combined_dataset,
        processing_class=sc_trainer.tokenizer,
        max_seq_length=max_length,
        dataset_text_field="text",
        packing=True,
        args=correction_training_args,
    )

    print("Starting enhanced self-correction training...")
    correction_trainer.train()

    # Save the final model
    save_path = f"{output_dir}/final_enhanced_model"
    correction_model.save_pretrained_merged(
        save_path,
        sc_trainer.tokenizer,
        save_method="merged_16bit"
    )

    # Save configuration and metadata
    config = {
        "model_name": model_name,
        "max_length": max_length,
        "instruction": instruction,
        "training_phases": {
            "phase1_epochs": num_epochs,
            "phase3_epochs": self_correction_epochs,
            "validation_examples_used": len(eval_dataset)
        },
        "correction_examples_created": len(correction_examples),
        "final_dataset_size": len(combined_dataset),
        "extraction_statistics": inference_trainer.extraction_stats
    }

    with open(os.path.join(output_dir, "enhanced_training_config.json"), "w") as f:
        json.dump(config, f, indent=2)

    print(f"Enhanced model successfully saved to {save_path}")
    print(f"Enhanced self-correction training completed!")
    
    return correction_model, sc_trainer.tokenizer


if __name__ == "__main__":
    # Configuration parameters
    data_path = "data/notes_train.csv"
    model_name = "unsloth/Phi-4-unsloth-bnb-4bit"
    output_dir = "./Phi-4-malnutrition-self-correction"
    max_length = 16384
    num_epochs = 5
    batch_size = 1
    self_correction_epochs = 2

    os.makedirs(output_dir, exist_ok=True)

    print(f"Starting enhanced self-correction training using {model_name}...")
    model, tokenizer = train_malnutrition_model_with_enhanced_self_correction(
        data_path=data_path,
        model_name=model_name,
        output_dir=output_dir,
        max_length=max_length,
        num_epochs=num_epochs,
        batch_size=batch_size,
        self_correction_epochs=self_correction_epochs
    )

    print("Enhanced self-correction training completed successfully!")
